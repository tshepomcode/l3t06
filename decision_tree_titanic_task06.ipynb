{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"mFPFeEu69axo"},"outputs":[],"source":["# Import packages \n","import pandas as pd\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.impute import SimpleImputer\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"e9C-68Ij9ayD","outputId":"cb08f206-100d-4a4c-f87d-13f2aec390cc"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PassengerId</th>\n","      <th>Survived</th>\n","      <th>Pclass</th>\n","      <th>Name</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>Ticket</th>\n","      <th>Fare</th>\n","      <th>Cabin</th>\n","      <th>Embarked</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Braund, Mr. Owen Harris</td>\n","      <td>male</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>A/5 21171</td>\n","      <td>7.2500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n","      <td>female</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>PC 17599</td>\n","      <td>71.2833</td>\n","      <td>C85</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>Heikkinen, Miss. Laina</td>\n","      <td>female</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>STON/O2. 3101282</td>\n","      <td>7.9250</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n","      <td>female</td>\n","      <td>35.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>113803</td>\n","      <td>53.1000</td>\n","      <td>C123</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Allen, Mr. William Henry</td>\n","      <td>male</td>\n","      <td>35.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>373450</td>\n","      <td>8.0500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PassengerId  Survived  Pclass  \\\n","0            1         0       3   \n","1            2         1       1   \n","2            3         1       3   \n","3            4         1       1   \n","4            5         0       3   \n","\n","                                                Name     Sex   Age  SibSp  \\\n","0                            Braund, Mr. Owen Harris    male  22.0      1   \n","1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n","2                             Heikkinen, Miss. Laina  female  26.0      0   \n","3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n","4                           Allen, Mr. William Henry    male  35.0      0   \n","\n","   Parch            Ticket     Fare Cabin Embarked  \n","0      0         A/5 21171   7.2500   NaN        S  \n","1      0          PC 17599  71.2833   C85        C  \n","2      0  STON/O2. 3101282   7.9250   NaN        S  \n","3      0            113803  53.1000  C123        S  \n","4      0            373450   8.0500   NaN        S  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["titanic_df = pd.read_csv(\"titanic.csv\")\n","titanic_df.head()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"CBV7Mx939ayO","outputId":"4e34ffef-0da5-49e5-d52c-379240000dc2"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 891 entries, 0 to 890\n","Data columns (total 12 columns):\n"," #   Column       Non-Null Count  Dtype  \n","---  ------       --------------  -----  \n"," 0   PassengerId  891 non-null    int64  \n"," 1   Survived     891 non-null    int64  \n"," 2   Pclass       891 non-null    int64  \n"," 3   Name         891 non-null    object \n"," 4   Sex          891 non-null    object \n"," 5   Age          714 non-null    float64\n"," 6   SibSp        891 non-null    int64  \n"," 7   Parch        891 non-null    int64  \n"," 8   Ticket       891 non-null    object \n"," 9   Fare         891 non-null    float64\n"," 10  Cabin        204 non-null    object \n"," 11  Embarked     889 non-null    object \n","dtypes: float64(2), int64(5), object(5)\n","memory usage: 83.7+ KB\n"]}],"source":["titanic_df.info()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"EhNnIWET9ayS"},"outputs":[],"source":["# You can also drop whichever other columns you'd like here\n","titanic_df.drop(\"Cabin\", axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"zFZ50hDg9aye"},"source":["### One-Hot Encoding\n","One-hot encoding is a technique used to ensure that categorical variables are better represented in the machine. Let's take a look at the \"Sex\" column"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"zxl62Q0-9ay1","outputId":"1b2e62ad-3707-424d-fc21-26d413fc9221"},"outputs":[{"data":{"text/plain":["array(['male', 'female'], dtype=object)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["titanic_df[\"Sex\"].unique()"]},{"cell_type":"markdown","metadata":{"id":"sXIAVO4Z9ay8"},"source":["Machine Learning classifiers don't know how to handle strings. As a result, you need to convert it into a categorical representation. There are two main ways to go about this:\n","\n","Label Encoding: Assigning, for example, 0 for \"male\" and 1 for \"female\". The problem here is it intrinsically makes one category \"larger than\" the other category.\n","\n","One-hot encoding: Assigning, for example, [1, 0] for \"male\" and [0, 1] for female. In this case, you have an array of size (n_categories,) and you represent a 1 in the correct index, and 0 elsewhere. In Pandas, this would show as extra columns. For example, rather than having a \"Sex\" column, it would be a \"Sex_male\" and \"Sex_female\" column. Then, if the person is male, it would simply show as a 1 in the \"Sex_male\" column and a 0 in the \"Sex_female\" column.\n","\n","There is a nice and easy method that does this in pandas: get_dummies()"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"hMLpI7IP9azC","outputId":"8973bd4a-7ecd-4abf-edda-ef096ce958d7"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PassengerId</th>\n","      <th>Survived</th>\n","      <th>Pclass</th>\n","      <th>Name</th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>Ticket</th>\n","      <th>Fare</th>\n","      <th>Embarked</th>\n","      <th>Sex_female</th>\n","      <th>Sex_male</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Braund, Mr. Owen Harris</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>A/5 21171</td>\n","      <td>7.2500</td>\n","      <td>S</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>PC 17599</td>\n","      <td>71.2833</td>\n","      <td>C</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>Heikkinen, Miss. Laina</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>STON/O2. 3101282</td>\n","      <td>7.9250</td>\n","      <td>S</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n","      <td>35.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>113803</td>\n","      <td>53.1000</td>\n","      <td>S</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Allen, Mr. William Henry</td>\n","      <td>35.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>373450</td>\n","      <td>8.0500</td>\n","      <td>S</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PassengerId  Survived  Pclass  \\\n","0            1         0       3   \n","1            2         1       1   \n","2            3         1       3   \n","3            4         1       1   \n","4            5         0       3   \n","\n","                                                Name   Age  SibSp  Parch  \\\n","0                            Braund, Mr. Owen Harris  22.0      1      0   \n","1  Cumings, Mrs. John Bradley (Florence Briggs Th...  38.0      1      0   \n","2                             Heikkinen, Miss. Laina  26.0      0      0   \n","3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  35.0      1      0   \n","4                           Allen, Mr. William Henry  35.0      0      0   \n","\n","             Ticket     Fare Embarked  Sex_female  Sex_male  \n","0         A/5 21171   7.2500        S       False      True  \n","1          PC 17599  71.2833        C        True     False  \n","2  STON/O2. 3101282   7.9250        S        True     False  \n","3            113803  53.1000        S        True     False  \n","4            373450   8.0500        S       False      True  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["titanic_df = pd.get_dummies(titanic_df, prefix=\"Sex\", columns=[\"Sex\"])\n","titanic_df.head()"]},{"cell_type":"markdown","metadata":{"id":"4b3ItGkb9azJ"},"source":["Now, we do the same to the \"Embarked\" column."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"5YEGpK3Y9azM","outputId":"4d16f7a8-987d-48ea-b043-8d7391e0491a"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PassengerId</th>\n","      <th>Survived</th>\n","      <th>Pclass</th>\n","      <th>Name</th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>Ticket</th>\n","      <th>Fare</th>\n","      <th>Sex_female</th>\n","      <th>Sex_male</th>\n","      <th>Embarked_C</th>\n","      <th>Embarked_Q</th>\n","      <th>Embarked_S</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Braund, Mr. Owen Harris</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>A/5 21171</td>\n","      <td>7.2500</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>PC 17599</td>\n","      <td>71.2833</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>Heikkinen, Miss. Laina</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>STON/O2. 3101282</td>\n","      <td>7.9250</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n","      <td>35.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>113803</td>\n","      <td>53.1000</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Allen, Mr. William Henry</td>\n","      <td>35.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>373450</td>\n","      <td>8.0500</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PassengerId  Survived  Pclass  \\\n","0            1         0       3   \n","1            2         1       1   \n","2            3         1       3   \n","3            4         1       1   \n","4            5         0       3   \n","\n","                                                Name   Age  SibSp  Parch  \\\n","0                            Braund, Mr. Owen Harris  22.0      1      0   \n","1  Cumings, Mrs. John Bradley (Florence Briggs Th...  38.0      1      0   \n","2                             Heikkinen, Miss. Laina  26.0      0      0   \n","3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  35.0      1      0   \n","4                           Allen, Mr. William Henry  35.0      0      0   \n","\n","             Ticket     Fare  Sex_female  Sex_male  Embarked_C  Embarked_Q  \\\n","0         A/5 21171   7.2500       False      True       False       False   \n","1          PC 17599  71.2833        True     False        True       False   \n","2  STON/O2. 3101282   7.9250        True     False       False       False   \n","3            113803  53.1000        True     False       False       False   \n","4            373450   8.0500       False      True       False       False   \n","\n","   Embarked_S  \n","0        True  \n","1       False  \n","2        True  \n","3        True  \n","4        True  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["titanic_df = pd.get_dummies(titanic_df, prefix=\"Embarked\", columns=[\"Embarked\"])\n","titanic_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["Create a Bagged, Random Forest, and Boosted tree for the titanic dataset in the same way that you created a regular Classification Tree."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Null Values in Training \n","PassengerId      0\n","Survived         0\n","Pclass           0\n","Name             0\n","Age            177\n","SibSp            0\n","Parch            0\n","Ticket           0\n","Fare             0\n","Sex_female       0\n","Sex_male         0\n","Embarked_C       0\n","Embarked_Q       0\n","Embarked_S       0\n","dtype: int64\n","\n","Duplicated values in train 0\n"]}],"source":["print('\\nNull Values in Training \\n{}'.format(titanic_df.isnull().sum()))\n","# print('\\nNull Values in Testing \\n{}'.format(titanic_df.isnull().sum()))\n","\n","print('\\nDuplicated values in train {}'.format(titanic_df.duplicated().sum()))\n","# print('Duplicated values in test {}'.format(titanic_df.duplicated().sum()))"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Null Values in Training \n","PassengerId    0\n","Survived       0\n","Pclass         0\n","Name           0\n","Age            0\n","SibSp          0\n","Parch          0\n","Ticket         0\n","Fare           0\n","Sex_female     0\n","Sex_male       0\n","Embarked_C     0\n","Embarked_Q     0\n","Embarked_S     0\n","dtype: int64\n"]}],"source":["# Drop rows with NaN values in 'Age'\n","titanic_df.dropna(subset=['Age'], inplace=True)\n","\n","# Display Age and other columns for null values.\n","print('\\nNull Values in Training \\n{}'.format(titanic_df.isnull().sum()))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set shape: (571, 14)\n","Development set shape: (71, 14)\n","Test set shape: (72, 14)\n"]},{"name":"stderr","output_type":"stream","text":["/home/mcode/school/hyperion/level3/l3t06/.venv/lib/python3.10/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n","  warnings.warn(\n"]},{"data":{"text/html":["<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(random_state=42)</pre></div></div></div></div></div>"],"text/plain":["GradientBoostingClassifier(random_state=42)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Select relevant features and target\n","features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\n","target = 'Survived'\n","\n","# Split the data into training, development, and test sets.\n","train_df, test_df = train_test_split(titanic_df, test_size=0.2, random_state=42)\n","dev_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)\n","\n","# Display the shapes of the datasets\n","print(f\"Training set shape: {train_df.shape}\")\n","print(f\"Development set shape: {dev_df.shape}\")\n","print(f\"Test set shape: {test_df.shape}\")\n","\n","# Split data into features (X) and target (y)\n","X_train = train_df[features]\n","y_train = train_df[target]\n","X_dev = dev_df[features]\n","y_dev = dev_df[target]\n","X_test = test_df[features]\n","y_test = test_df[target]\n","\n","# Initialize models\n","bagging_model = BaggingClassifier(base_estimator=None, n_estimators=100, random_state=42)\n","random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","boosted_tree_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n","\n","# Fit models to training data\n","bagging_model.fit(X_train, y_train)\n","random_forest_model.fit(X_train, y_train)\n","boosted_tree_model.fit(X_train, y_train)\n"]},{"cell_type":"markdown","metadata":{},"source":["From the Random Forest model, we will determine which of the features is the one that contributes the most to predicting whether a passenger survives or not.<br>\n","We will examine the feature importance scores provided by the model."]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Feature Importance:\n","Age: 0.27484997302928105\n","Fare: 0.23409032243796835\n","Sex_male: 0.1635178293964902\n","Sex_female: 0.13415222366607651\n","Pclass: 0.0855321380207055\n","SibSp: 0.04103556058185477\n","Parch: 0.036579537105091584\n","Embarked_C: 0.0151580335334631\n","Embarked_S: 0.011009724821406013\n","Embarked_Q: 0.004074657407663029\n"]}],"source":["# Get feature importance scores from the Random Forest model\n","feature_importances = random_forest_model.feature_importances_\n","\n","# Create a dictionary to pair features with their importance scores\n","feature_importance_dict = dict(zip(features, feature_importances))\n","\n","# Sort the features based on their importance scores in descending order\n","sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n","\n","# Display the sorted feature importance\n","print(\"Feature Importance:\")\n","for feature, importance in sorted_feature_importance:\n","    print(f\"{feature}: {importance}\")\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["<b>Age seems to be the feature with the most importance contributing to the survival of a passenger.</b>"]},{"cell_type":"markdown","metadata":{},"source":["Tuning the parameters n_estimators and max_depth for the Random Forest model.\n","\n","We will go through a range of values for these parameters and find the combination that performs the best."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["n_estimators=50, max_depth=None, Accuracy on Development Set: 0.8028\n","n_estimators=50, max_depth=10, Accuracy on Development Set: 0.8169\n","n_estimators=50, max_depth=20, Accuracy on Development Set: 0.8028\n","n_estimators=50, max_depth=30, Accuracy on Development Set: 0.8028\n","n_estimators=100, max_depth=None, Accuracy on Development Set: 0.8028\n","n_estimators=100, max_depth=10, Accuracy on Development Set: 0.8028\n","n_estimators=100, max_depth=20, Accuracy on Development Set: 0.8169\n","n_estimators=100, max_depth=30, Accuracy on Development Set: 0.8028\n","n_estimators=150, max_depth=None, Accuracy on Development Set: 0.8028\n","n_estimators=150, max_depth=10, Accuracy on Development Set: 0.8028\n","n_estimators=150, max_depth=20, Accuracy on Development Set: 0.8028\n","n_estimators=150, max_depth=30, Accuracy on Development Set: 0.8028\n","\n","Best Hyperparameters:\n","n_estimators=50, max_depth=10, Best Accuracy on Development Set: 0.8169\n"]},{"data":{"text/html":["<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=10, n_estimators=50, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=10, n_estimators=50, random_state=42)</pre></div></div></div></div></div>"],"text/plain":["RandomForestClassifier(max_depth=10, n_estimators=50, random_state=42)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Define the hyperparameter values to try\n","n_estimators_values = [50, 100, 150]\n","max_depth_values = [None, 10, 20, 30]\n","\n","best_accuracy = 0.0\n","best_n_estimators = None\n","best_max_depth = None\n","\n","# Iterate over hyperparameter values\n","for n_estimators in n_estimators_values:\n","    for max_depth in max_depth_values:\n","        # Create Random Forest model with current hyperparameters\n","        random_forest_model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n","        \n","        # Fit the model on the training data\n","        random_forest_model.fit(X_train, y_train)\n","        \n","        # Evaluate the model on the development set\n","        dev_accuracy = random_forest_model.score(X_dev, y_dev)\n","        \n","        # Print or store the results\n","        print(f\"n_estimators={n_estimators}, max_depth={max_depth}, Accuracy on Development Set: {dev_accuracy:.4f}\")\n","        \n","        # Check if this set of hyperparameters gives better accuracy\n","        if dev_accuracy > best_accuracy:\n","            best_accuracy = dev_accuracy\n","            best_n_estimators = n_estimators\n","            best_max_depth = max_depth\n","\n","# Print the best hyperparameters found\n","print(\"\\nBest Hyperparameters:\")\n","print(f\"n_estimators={best_n_estimators}, max_depth={best_max_depth}, Best Accuracy on Development Set: {best_accuracy:.4f}\")\n","\n","# Train the final model with the best hyperparameters on the full training set\n","final_model = RandomForestClassifier(n_estimators=best_n_estimators, max_depth=best_max_depth, random_state=42)\n","final_model.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["<b>Given the outcome below, the best model would be the Random Forest with the previous result after tuning the n_estimators = 50 and max_depth = 10.<b>"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Bagging Model Accuracy on Development Set: 0.8028\n","Random Forest Model Accuracy on Development Set: 0.8028\n","Boosted Tree Model Accuracy on Development Set: 0.8028\n","\n","Best Performing Model: Bagging\n","Best Accuracy on Development Set: 0.8028\n","Hyperparameters: n_estimators=100\n"]}],"source":["# Predictions on development set\n","bagging_dev_predictions = bagging_model.predict(X_dev)\n","random_forest_dev_predictions = random_forest_model.predict(X_dev)\n","boosted_tree_dev_predictions = boosted_tree_model.predict(X_dev)\n","\n","# Calculate accuracy for each model on the development set\n","bagging_dev_accuracy = accuracy_score(y_dev, bagging_dev_predictions)\n","random_forest_dev_accuracy = accuracy_score(y_dev, random_forest_dev_predictions)\n","boosted_tree_dev_accuracy = accuracy_score(y_dev, boosted_tree_dev_predictions)\n","\n","# Report accuracy for each model\n","print(f\"Bagging Model Accuracy on Development Set: {bagging_dev_accuracy:.4f}\")\n","print(f\"Random Forest Model Accuracy on Development Set: {random_forest_dev_accuracy:.4f}\")\n","print(f\"Boosted Tree Model Accuracy on Development Set: {boosted_tree_dev_accuracy:.4f}\")\n","\n","# Identify the best-performing model\n","best_model = max([\n","    (\"Bagging\", bagging_dev_accuracy),\n","    (\"Random Forest\", random_forest_dev_accuracy),\n","    (\"Boosted Tree\", boosted_tree_dev_accuracy)\n","], key=lambda x: x[1])\n","\n","# Report the best-performing model and its hyperparameters\n","print(f\"\\nBest Performing Model: {best_model[0]}\")\n","print(f\"Best Accuracy on Development Set: {best_model[1]:.4f}\")\n","if best_model[0] == \"Bagging\":\n","    print(f\"Hyperparameters: n_estimators={bagging_model.n_estimators}\")\n","elif best_model[0] == \"Random Forest\":\n","    print(f\"Hyperparameters: n_estimators={random_forest_model.n_estimators}, max_depth={random_forest_model.max_depth}\")\n","elif best_model[0] == \"Boosted Tree\":\n","    print(f\"Hyperparameters: n_estimators={boosted_tree_model.n_estimators}, max_depth={boosted_tree_model.max_depth}\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"cvML","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"1904059d3876957b542b45423f2a26c6c4608f5e11cc75420e543fa77f94b066"}}},"nbformat":4,"nbformat_minor":0}
